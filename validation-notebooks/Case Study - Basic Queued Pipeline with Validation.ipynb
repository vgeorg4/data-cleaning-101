{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Queued Pipeline with Dask Distributed + Schema Validation\n",
    "\n",
    "In this case study, we will take a look at a pipeline which takes in Router environment variables (like temperature and fan RPM) and determines whether they are outside of normal ranges. \n",
    "\n",
    "We will define schema in [Voluptuous](https://github.com/alecthomas/voluptuous) to set the threshholds we expect to see and use [Dask Distributed](http://distributed.readthedocs.io/en/latest/index.html) to schedule and distribute the work across several workers. We use [dataset](https://dataset.readthedocs.io/en/latest/) to make a quick `sqlite3` database to store our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import dataset\n",
    "import sys\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "from queue import Queue\n",
    "from queue_example import generate_example, generate_machine_db\n",
    "from distributed import Client\n",
    "from voluptuous import Schema, Required, Range, All, ALLOW_EXTRA\n",
    "from voluptuous.error import MultipleInvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(0)\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We set up a Queue and start adding events via another thread. This will keep running until we stop the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = Queue()\n",
    "db = dataset.connect('sqlite:///output_db.db')\n",
    "table = db['readings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_q):\n",
    "    while True:\n",
    "        input_q.put(generate_example())\n",
    "        sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "load_thread = Thread(target=load_data, args=(queue,))\n",
    "load_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then, we define our schema in Voluptuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema({\n",
    "    Required('AmbientTemp'): All(float, Range(min=3, max=40)),\n",
    "    Required('Fan'): All(int, Range(min=100, max=2000)),\n",
    "    Required('CpuTemp'): All(float, Range(min=5, max=50)),\n",
    "}, extra=ALLOW_EXTRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we need to start our scheduler and workers.\n",
    "\n",
    "### Commands (if you are in environment you installed distributed in AND this folder validation-notebooks): \n",
    "    - To start the scheduler: dask-scheduler\n",
    "    - Then, in a terminal, navigate to validation-notebooks and run an export to add the path to your PYTHONPATH\n",
    "        i.e. export PYTHONPATH=PYTHONPATH:/path/to/validation/notebooks\n",
    "    - In that same terminal, start a worker: dask-worker SCHEDULER_IP:SCHEDULER_PORT (most often 127.0.0.1:8786)\n",
    "\n",
    "To view the Bokeh application: click on the Web UI link (usually: http://127.0.0.1:8787/status/ )\n",
    "\n",
    "#### Note: you need to start your workers in *this* directory, or copy the `queue_example.py` file to an accessible place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client('127.0.0.1:8786')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can define our pipeline functions:\n",
    "\n",
    "1. Test the schema adding warnings if we find schema failures.\n",
    "2. Add some extra machine information from our machine database*. (*just a dict, but use your imagination)\n",
    "3. Insert our reading into our database of readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_schema(reading):\n",
    "    try:\n",
    "        schema(reading)\n",
    "        reading['warning'] = False\n",
    "    except MultipleInvalid as e:\n",
    "        logger.warning('SCHEMA: Issue with router %s (%s)', \n",
    "                       reading.get('MachineId'), e)\n",
    "        reading['warning'] = True\n",
    "    return reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_machine_info(reading):\n",
    "    mdb = generate_machine_db()\n",
    "    reading['brand'] = mdb[reading['MachineId']]\n",
    "    return reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reading(reading):\n",
    "    db = dataset.connect('sqlite:///output_db.db',\n",
    "                        engine_kwargs={'connect_args': \n",
    "                                       {'check_same_thread':False}})\n",
    "    table = db['readings']\n",
    "    reading['processed_at'] = datetime.now()\n",
    "    table.insert(reading)\n",
    "    return reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To begin, we scatter the queue from the data to our workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_q = client.scatter(queue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, we create a series of `map` functions, passing the futures objects to the next step of the pipeline. At the end we `gather` the data into one queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_q = client.map(test_schema, remote_q)\n",
    "info_q = client.map(add_machine_info, schema_q)\n",
    "insert_q = client.map(add_reading, info_q)\n",
    "final = client.gather(insert_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Then, we can collect the data using `get`\n",
    "\n",
    "#### Note: you can watch errors and logging in the worker processes. And make sure to check the Bokeh Status Web UI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "while count < 40:\n",
    "    item = final.get()\n",
    "    print(item)\n",
    "    print('Queue size: ', queue.qsize())\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = dataset.connect('sqlite:///output_db.db')\n",
    "table = db['readings']\n",
    "table.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings = db.query('''SELECT COUNT(*) as cnt FROM readings \n",
    "                       where warning == 1''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(warnings)[0].get('cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise: did you see any other errors (or can you spot a potential error when checking the `queue_example.py` file?)  How might we prevent the error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
